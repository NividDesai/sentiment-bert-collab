# Sentiment Analysis with BERT



A production-ready sentiment analysis system using BERT (Bidirectional Encoder Representations from Transformers) for binary sentiment classification of app reviews.

## ğŸ“‹ Project Overview

This project implements a complete MLOps pipeline for sentiment analysis, including:
- Data extraction and preprocessing
- BERT-based model training
- Comprehensive testing and CI/CD
- Model evaluation and visualization
- Inference capabilities

**Team Members:** Nivid DESAI & Shreya PALLISSERY

## ğŸŒŸ Features

- âœ… **BERT-based Classification**: Uses pre-trained `bert-base-uncased` for transfer learning
- âœ… **Automated Testing**: Comprehensive unit and integration tests with **100% coverage** ğŸ¯
- âœ… **CI/CD Pipeline**: GitHub Actions for automated testing and quality checks
- âœ… **Code Quality**: Pre-commit hooks, Black formatting, and Ruff linting
- âœ… **Reproducibility**: Fixed random seeds for consistent results
- âœ… **Visualization**: Confusion matrix and metrics reporting
- âœ… **Production Ready**: Inference script for deployment

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- pip package manager
- (Optional) CUDA-capable GPU for faster training

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/NividDESAI/sentiment-bert-collab.git
cd sentiment-bert-collab
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up pre-commit hooks** (optional but recommended)
```bash
pre-commit install
```

## ğŸ“Š Dataset

The project uses app review data with the following columns:
- `content`: Review text
- `score`: Rating (1-5)
- Derived `label`: Binary sentiment (0=negative for score<3, 1=positive for scoreâ‰¥3)

Dataset statistics:
- Total samples: 14,078 reviews
- Binary classification: Positive/Negative sentiment

## ğŸ¯ Usage

### Training the Model

Train a sentiment classifier with default settings:

```bash
python train.py
```

With custom parameters:

```bash
python train.py --data dataset.csv \
                --model-name bert-base-uncased \
                --epochs 3 \
                --batch-size 8 \
                --max-length 128 \
                --output-dir ./outputs
```

**Training arguments:**
- `--data`: Path to CSV dataset (default: `dataset.csv`)
- `--model-name`: HuggingFace model identifier (default: `bert-base-uncased`)
- `--epochs`: Number of training epochs (default: 3)
- `--batch-size`: Training batch size (default: 8)
- `--max-length`: Maximum sequence length (default: 128)
- `--output-dir`: Output directory for model and results (default: `./outputs`)
- `--seed`: Random seed for reproducibility (default: 42)

**Output files:**
- `outputs/final_model/`: Saved model checkpoint
- `outputs/metrics.txt`: Performance metrics
- `outputs/confusion_matrix.png`: Confusion matrix visualization

### Running Inference

Predict sentiment for a single text:

```bash
python inference.py --text "This app is amazing!"
```

Batch prediction from file:

```bash
# Create a file with one review per line
echo "Great app, highly recommend" > samples.txt
echo "Terrible experience, waste of money" >> samples.txt

python inference.py --file samples.txt
```

### Running Tests

Run all tests:

```bash
pytest tests/unit/ -v
```

Run with coverage report (**100% coverage achieved!** ğŸ‰):

```bash
pytest tests/unit/ --cov=src --cov-report=term-missing --cov-report=html
# Coverage: 100% (111/111 statements)
# 40 passed, 4 skipped
```

Run specific test categories:

```bash
# Unit tests only
pytest tests/unit/test_data_extraction.py -v

# Integration tests
pytest tests/unit/test_train_integration.py -v
```

## ğŸ—ï¸ Project Structure

```
sentiment-bert-collab/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_extraction.py      # Data loading and preprocessing
â”‚   â”œâ”€â”€ model.py                # Model definition and training
â”‚   â”œâ”€â”€ tokenize_helper.py      # Tokenization utilities
â”‚   â””â”€â”€ utils.py                # Helper functions
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ fixtures/               # Test data
â”‚   â””â”€â”€ unit/                   # Unit tests
â”‚       â”œâ”€â”€ test_data_extraction.py
â”‚       â”œâ”€â”€ test_model.py
â”‚       â”œâ”€â”€ test_tokenize.py
â”‚       â”œâ”€â”€ test_utils.py
â”‚       â””â”€â”€ test_train_integration.py
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml              # CI/CD pipeline
â”œâ”€â”€ report/
â”‚   â””â”€â”€ project_report.md       # Detailed project documentation
â”œâ”€â”€ train.py                    # Main training script
â”œâ”€â”€ inference.py                # Inference script
â”œâ”€â”€ dataset.csv                 # Training data
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ“ˆ Model Performance

### Metrics

After training for 3 epochs on the full dataset:

| Metric      | Value  |
|-------------|--------|
| Accuracy    | ~0.92  |
| Precision   | ~0.91  |
| Recall      | ~0.92  |
| F1 Score    | ~0.91  |

### Training Details

- **Model**: BERT base uncased (110M parameters)
- **Optimizer**: AdamW
- **Learning rate**: 5e-5 (default from Trainer)
- **Batch size**: 8
- **Max sequence length**: 128 tokens
- **Training time**: ~30 minutes on GPU (varies by hardware)

## ğŸ”§ Development

### Code Quality

The project enforces code quality through:

1. **Black**: Code formatting (line length: 100)
```bash
black src/ tests/
```

2. **Ruff**: Fast Python linter
```bash
ruff check src/ tests/
```

3. **Pre-commit hooks**: Automatic checks before commits
```bash
pre-commit run --all-files
```

### Testing Strategy

- **Unit tests**: Test individual functions and components
- **Integration tests**: Test end-to-end pipeline
- **Coverage achieved**: **100% code coverage** ğŸ¯
  - All 6 modules: 100% coverage
  - 44 total tests (40 passed, 4 skipped)
  - See `COVERAGE_ACHIEVEMENT.md` for details
- **CI/CD**: Automated testing on push/PR





